{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape';\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n  let out;\n  const intermediates = [];\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n  const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n  if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') || !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') || !reshapeWillBeExpensive) {\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] : xShape[0] * xShape[2] * xShape[3];\n    const xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: [1, targetShape, convInfo.inChannels]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    const result = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  } else {\n    // Following optimization is specific to packed |x| with odd row count\n    // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n    // we avoid expensive packed 2x2 reshape by padding row count to next,\n    // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n    // even-rows tensor before the operation and, after the batchMatMul,\n    // fix the even-rows result to have odd number of rows.\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * (xShape[2] + 1) : xShape[0] * xShape[2] * (xShape[3] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing row count, after batchMatMul->...->compileProgram leads to\n    // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even row count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed');\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  }\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return out;\n}\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n  const intermediates = [];\n  const xSqueezed = reshape({\n    inputs: {\n      x\n    },\n    backend,\n    attrs: {\n      shape: x.shape.slice(1)\n    }\n  });\n  const w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32');\n  const im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend,\n    attrs: {\n      shape: [1, x2ColShape[0], x2ColShape[1]]\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs = [im2ColReshaped, w2Row];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const outShape = isChannelsLast ? [1, outHeight, outWidth, convInfo.outChannels] : [1, convInfo.outChannels, outHeight, outWidth];\n  const out = reshape({\n    inputs: {\n      x: product\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(product);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return out;\n}","map":{"version":3,"names":["env","util","Im2ColPackedProgram","mapActivationToShaderProgram","MatMulPackedProgram","webgl_util","batchMatMulImpl","MATMUL_SHARED_DIM_THRESHOLD","identity","reshape","conv2dByMatMul","x","filter","convInfo","backend","bias","preluActivationWeights","leakyreluAlpha","activation","xShape","shape","xTexData","texData","get","dataId","sharedMatMulDim","inChannels","outerShapeX","outerShapeFilter","outChannels","isChannelsLast","dataFormat","transposeA","transposeB","out","intermediates","batchMatMulWillBeUnpacked","reshapeWillBeExpensive","isPacked","getBool","targetShape","xReshaped","inputs","attrs","filterReshaped","result","a","b","outShape","push","dtype","originalXTexDataShape","slice","length","assert","isReshapeFree","pointwiseConv","pointwiseConvTexData","i","disposeIntermediateTensorInfo","conv2dWithIm2Row","filterWidth","filterHeight","outWidth","outHeight","sharedDim","numCols","x2ColShape","xSqueezed","w2Row","sizeFromShape","im2ColProgram","im2Col","runWebGLProgram","im2ColReshaped","hasBias","hasPreluActivationWeights","hasLeakyreluAlpha","fusedActivation","matmulProgram","$leakyreluAlpha","makeTensorInfo","createScalarValue","product"],"sources":["C:\\Users\\reddy\\Documents\\Projects\\Engagement Tracker\\engagement-tracker-react\\node_modules\\@tensorflow\\tfjs-backend-webgl\\src\\kernels\\Conv2D_impl.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, env, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {Im2ColPackedProgram} from '../im2col_packed_gpu';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\n\nimport {batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD} from './BatchMatMul_impl';\nimport {identity} from './Identity';\nimport {reshape} from './Reshape';\n\ntype Conv2DConfig = {\n  x: TensorInfo,\n  filter: TensorInfo,\n  convInfo: backend_util.Conv2DInfo,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n\n  let out: TensorInfo;\n  const intermediates: TensorInfo[] = [];\n\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  const batchMatMulWillBeUnpacked =\n      (outerShapeX === 1 || outerShapeFilter === 1) &&\n      sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n  const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n\n  if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') ||\n      !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ||\n      !reshapeWillBeExpensive) {\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] :\n                                         xShape[0] * xShape[2] * xShape[3];\n    const xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {shape: [1, targetShape, convInfo.inChannels]}\n    });\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    const result = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    out = reshape(\n        {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  } else {\n    // Following optimization is specific to packed |x| with odd row count\n    // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n    // we avoid expensive packed 2x2 reshape by padding row count to next,\n    // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n    // even-rows tensor before the operation and, after the batchMatMul,\n    // fix the even-rows result to have odd number of rows.\n    const targetShape = isChannelsLast ?\n        xShape[0] * xShape[1] * (xShape[2] + 1) :\n        xShape[0] * xShape[2] * (xShape[3] + 1);\n    const xReshaped: TensorInfo = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing row count, after batchMatMul->...->compileProgram leads to\n    // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even row count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(\n        webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape),\n        () => `packed reshape ${xTexData.shape} to ${\n            xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(\n        pointwiseConvTexData.isPacked,\n        () => 'batchMatMul result is expected to be packed');\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n\n    out = identity({inputs: {x: pointwiseConv}, backend});\n    out.shape = convInfo.outShape;\n\n    intermediates.push(pointwiseConv);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n\n  const isChannelsLast = dataFormat === 'channelsLast';\n\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n\n  const intermediates: TensorInfo[] = [];\n\n  const xSqueezed =\n      reshape({inputs: {x}, backend, attrs: {shape: x.shape.slice(1)}});\n  const w2Row = reshape({\n    inputs: {x: filter},\n    backend,\n    attrs: {shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]}\n  });\n\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n\n  const im2ColProgram =\n      new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32');\n  const im2ColReshaped = reshape({\n    inputs: {x: im2Col},\n    backend,\n    attrs: {shape: [1, x2ColShape[0], x2ColShape[1]]}\n  });\n\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation =\n      activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(\n      im2ColReshaped.shape as [number, number, number],\n      w2Row.shape as [number, number, number],\n      [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias,\n      fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs: TensorInfo[] = [im2ColReshaped, w2Row];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo(\n        [], 'float32',\n        util.createScalarValue(leakyreluAlpha as {} as 'float32', 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n\n  const outShape = isChannelsLast ?\n      [1, outHeight, outWidth, convInfo.outChannels] :\n      [1, convInfo.outChannels, outHeight, outWidth];\n  const out =\n      reshape({inputs: {x: product}, backend, attrs: {shape: outShape}});\n\n  intermediates.push(product);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsBA,GAAG,EAAcC,IAAI,QAAO,uBAAuB;AAGzE,SAAQC,mBAAmB,QAAO,sBAAsB;AACxD,SAAQC,4BAA4B,QAAO,oCAAoC;AAC/E,SAAQC,mBAAmB,QAAO,sBAAsB;AACxD,OAAO,KAAKC,UAAU,MAAM,eAAe;AAE3C,SAAQC,eAAe,EAAEC,2BAA2B,QAAO,oBAAoB;AAC/E,SAAQC,QAAQ,QAAO,YAAY;AACnC,SAAQC,OAAO,QAAO,WAAW;AAajC;AACA;AACA;AACA,OAAM,SAAUC,cAAcA,CAAC;EAC7BC,CAAC;EACDC,MAAM;EACNC,QAAQ;EACRC,OAAO;EACPC,IAAI,GAAG,IAAI;EACXC,sBAAsB,GAAG,IAAI;EAC7BC,cAAc,GAAG,CAAC;EAClBC,UAAU,GAAG;AAAI,CACJ;EACb;EACA;EACA,MAAMC,MAAM,GAAGR,CAAC,CAACS,KAAK;EACtB,MAAMC,QAAQ,GAAGP,OAAO,CAACQ,OAAO,CAACC,GAAG,CAACZ,CAAC,CAACa,MAAM,CAAC;EAC9C,MAAMC,eAAe,GAAGZ,QAAQ,CAACa,UAAU;EAC3C,MAAMC,WAAW,GAAGR,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC;EACrD,MAAMS,gBAAgB,GAAGf,QAAQ,CAACgB,WAAW;EAC7C,MAAMC,cAAc,GAAGjB,QAAQ,CAACkB,UAAU,KAAK,cAAc;EAC7D,MAAMC,UAAU,GAAG,KAAK;EACxB,MAAMC,UAAU,GAAG,KAAK;EAExB,IAAIC,GAAe;EACnB,MAAMC,aAAa,GAAiB,EAAE;EAEtC;EACA;EACA,MAAMC,yBAAyB,GAC3B,CAACT,WAAW,KAAK,CAAC,IAAIC,gBAAgB,KAAK,CAAC,KAC5CH,eAAe,GAAGlB,2BAA2B;EACjD,MAAM8B,sBAAsB,GAAGlB,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,CAAC,CAACE,QAAQ,CAACiB,QAAQ;EAEzE,IAAIF,yBAAyB,IAAI,CAACpC,GAAG,EAAE,CAACuC,OAAO,CAAC,qBAAqB,CAAC,IAClE,CAACvC,GAAG,EAAE,CAACuC,OAAO,CAAC,8BAA8B,CAAC,IAC9C,CAACF,sBAAsB,EAAE;IAC3B,MAAMG,WAAW,GAAGV,cAAc,GAAGX,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,GACjCA,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC;IACtE,MAAMsB,SAAS,GAAGhC,OAAO,CAAC;MACxBiC,MAAM,EAAE;QAAC/B;MAAC,CAAC;MACXG,OAAO;MACP6B,KAAK,EAAE;QAACvB,KAAK,EAAE,CAAC,CAAC,EAAEoB,WAAW,EAAE3B,QAAQ,CAACa,UAAU;MAAC;KACrD,CAAC;IACF,MAAMkB,cAAc,GAAGnC,OAAO,CAAC;MAC7BiC,MAAM,EAAE;QAAC/B,CAAC,EAAEC;MAAM,CAAC;MACnBE,OAAO;MACP6B,KAAK,EAAE;QAACvB,KAAK,EAAE,CAAC,CAAC,EAAEP,QAAQ,CAACa,UAAU,EAAEb,QAAQ,CAACgB,WAAW;MAAC;KAC9D,CAAC;IACF,MAAMgB,MAAM,GAAGvC,eAAe,CAAC;MAC7BwC,CAAC,EAAEL,SAAS;MACZM,CAAC,EAAEH,cAAc;MACjBZ,UAAU;MACVC,UAAU;MACVnB,OAAO;MACPC,IAAI;MACJG,UAAU;MACVF,sBAAsB;MACtBC;KACD,CAAC;IAEFiB,GAAG,GAAGzB,OAAO,CACT;MAACiC,MAAM,EAAE;QAAC/B,CAAC,EAAEkC;MAAM,CAAC;MAAE/B,OAAO;MAAE6B,KAAK,EAAE;QAACvB,KAAK,EAAEP,QAAQ,CAACmC;MAAQ;IAAC,CAAC,CAAC;IAEtEb,aAAa,CAACc,IAAI,CAACR,SAAS,CAAC;IAC7BN,aAAa,CAACc,IAAI,CAACL,cAAc,CAAC;IAClCT,aAAa,CAACc,IAAI,CAACJ,MAAM,CAAC;GAC3B,MAAM;IACL;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAML,WAAW,GAAGV,cAAc,GAC9BX,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,IAAIA,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,GACvCA,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,IAAIA,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC;IAC3C,MAAMsB,SAAS,GAAe;MAC5BjB,MAAM,EAAEb,CAAC,CAACa,MAAM;MAChBJ,KAAK,EAAE,CAAC,CAAC,EAAEoB,WAAW,EAAE3B,QAAQ,CAACa,UAAU,CAAC;MAC5CwB,KAAK,EAAEvC,CAAC,CAACuC;KACV;IACD;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAMC,qBAAqB,GAAG9B,QAAQ,CAACD,KAAK;IAC5CC,QAAQ,CAACD,KAAK,GAAGC,QAAQ,CAACD,KAAK,CAACgC,KAAK,EAAE;IACvC/B,QAAQ,CAACD,KAAK,CAACC,QAAQ,CAACD,KAAK,CAACiC,MAAM,GAAG,CAAC,CAAC,EAAE;IAC3CpD,IAAI,CAACqD,MAAM,CACPjD,UAAU,CAACkD,aAAa,CAAClC,QAAQ,CAACD,KAAK,EAAEqB,SAAS,CAACrB,KAAK,CAAC,EACzD,MAAM,kBAAkBC,QAAQ,CAACD,KAAK,OAClCqB,SAAS,CAACrB,KAAK,aAAa,CAAC;IACrC,MAAMwB,cAAc,GAAGnC,OAAO,CAAC;MAC7BiC,MAAM,EAAE;QAAC/B,CAAC,EAAEC;MAAM,CAAC;MACnBE,OAAO;MACP6B,KAAK,EAAE;QAACvB,KAAK,EAAE,CAAC,CAAC,EAAEP,QAAQ,CAACa,UAAU,EAAEb,QAAQ,CAACgB,WAAW;MAAC;KAC9D,CAAC;IACFM,aAAa,CAACc,IAAI,CAACL,cAAc,CAAC;IAClC,MAAMY,aAAa,GAAGlD,eAAe,CAAC;MACpCwC,CAAC,EAAEL,SAAS;MACZM,CAAC,EAAEH,cAAc;MACjB9B,OAAO;MACPkB,UAAU;MACVC,UAAU;MACVlB,IAAI;MACJG,UAAU;MACVF,sBAAsB;MACtBC;KACD,CAAC;IAEF,MAAMwC,oBAAoB,GAAG3C,OAAO,CAACQ,OAAO,CAACC,GAAG,CAACiC,aAAa,CAAChC,MAAM,CAAC;IACtEvB,IAAI,CAACqD,MAAM,CACPG,oBAAoB,CAACnB,QAAQ,EAC7B,MAAM,6CAA6C,CAAC;IACxD;IACAjB,QAAQ,CAACD,KAAK,GAAG+B,qBAAqB;IACtC;IACA;IACAM,oBAAoB,CAACrC,KAAK,GAAGP,QAAQ,CAACmC,QAAQ;IAE9Cd,GAAG,GAAG1B,QAAQ,CAAC;MAACkC,MAAM,EAAE;QAAC/B,CAAC,EAAE6C;MAAa,CAAC;MAAE1C;IAAO,CAAC,CAAC;IACrDoB,GAAG,CAACd,KAAK,GAAGP,QAAQ,CAACmC,QAAQ;IAE7Bb,aAAa,CAACc,IAAI,CAACO,aAAa,CAAC;;EAGnC,KAAK,MAAME,CAAC,IAAIvB,aAAa,EAAE;IAC7BrB,OAAO,CAAC6C,6BAA6B,CAACD,CAAC,CAAC;;EAG1C,OAAOxB,GAAG;AACZ;AAEA;AACA;AACA,OAAM,SAAU0B,gBAAgBA,CAAC;EAC/BjD,CAAC;EACDC,MAAM;EACNC,QAAQ;EACRC,OAAO;EACPC,IAAI,GAAG,IAAI;EACXC,sBAAsB,GAAG,IAAI;EAC7BC,cAAc,GAAG,CAAC;EAClBC,UAAU,GAAG;AAAI,CACJ;EACb;EACA;EACA;EACA;EACA;EACA;EACA,MAAM;IACJ2C,WAAW;IACXC,YAAY;IACZpC,UAAU;IACVqC,QAAQ;IACRC,SAAS;IACTjC;EAAU,CACX,GAAGlB,QAAQ;EAEZ,MAAMiB,cAAc,GAAGC,UAAU,KAAK,cAAc;EAEpD,MAAMkC,SAAS,GAAGJ,WAAW,GAAGC,YAAY,GAAGpC,UAAU;EACzD,MAAMwC,OAAO,GAAGF,SAAS,GAAGD,QAAQ;EACpC,MAAMI,UAAU,GAAG,CAACF,SAAS,EAAEC,OAAO,CAAC;EACvC,MAAMlC,UAAU,GAAG,IAAI;EACvB,MAAMC,UAAU,GAAG,KAAK;EAExB,MAAME,aAAa,GAAiB,EAAE;EAEtC,MAAMiC,SAAS,GACX3D,OAAO,CAAC;IAACiC,MAAM,EAAE;MAAC/B;IAAC,CAAC;IAAEG,OAAO;IAAE6B,KAAK,EAAE;MAACvB,KAAK,EAAET,CAAC,CAACS,KAAK,CAACgC,KAAK,CAAC,CAAC;IAAC;EAAC,CAAC,CAAC;EACrE,MAAMiB,KAAK,GAAG5D,OAAO,CAAC;IACpBiC,MAAM,EAAE;MAAC/B,CAAC,EAAEC;IAAM,CAAC;IACnBE,OAAO;IACP6B,KAAK,EAAE;MAACvB,KAAK,EAAE,CAAC,CAAC,EAAE6C,SAAS,EAAEhE,IAAI,CAACqE,aAAa,CAAC1D,MAAM,CAACQ,KAAK,CAAC,GAAG6C,SAAS;IAAC;GAC5E,CAAC;EAEF9B,aAAa,CAACc,IAAI,CAACmB,SAAS,CAAC;EAC7BjC,aAAa,CAACc,IAAI,CAACoB,KAAK,CAAC;EAEzB,MAAME,aAAa,GACf,IAAIrE,mBAAmB,CAACiE,UAAU,EAAEC,SAAS,CAAChD,KAAK,EAAEP,QAAQ,CAAC;EAClE,MAAM2D,MAAM,GAAG1D,OAAO,CAAC2D,eAAe,CAACF,aAAa,EAAE,CAACH,SAAS,CAAC,EAAE,SAAS,CAAC;EAC7E,MAAMM,cAAc,GAAGjE,OAAO,CAAC;IAC7BiC,MAAM,EAAE;MAAC/B,CAAC,EAAE6D;IAAM,CAAC;IACnB1D,OAAO;IACP6B,KAAK,EAAE;MAACvB,KAAK,EAAE,CAAC,CAAC,EAAE+C,UAAU,CAAC,CAAC,CAAC,EAAEA,UAAU,CAAC,CAAC,CAAC;IAAC;GACjD,CAAC;EAEFhC,aAAa,CAACc,IAAI,CAACuB,MAAM,CAAC;EAC1BrC,aAAa,CAACc,IAAI,CAACyB,cAAc,CAAC;EAElC,MAAMC,OAAO,GAAG5D,IAAI,IAAI,IAAI;EAC5B,MAAM6D,yBAAyB,GAAG5D,sBAAsB,IAAI,IAAI;EAChE,MAAM6D,iBAAiB,GAAG3D,UAAU,KAAK,WAAW;EACpD,MAAM4D,eAAe,GACjB5D,UAAU,GAAGf,4BAA4B,CAACe,UAAU,EAAE,IAAI,CAAC,GAAG,IAAI;EACtE,MAAM6D,aAAa,GAAG,IAAI3E,mBAAmB,CACzCsE,cAAc,CAACtD,KAAiC,EAChDiD,KAAK,CAACjD,KAAiC,EACvC,CAAC,CAAC,EAAE8C,OAAO,EAAErD,QAAQ,CAACgB,WAAW,CAAC,EAAEG,UAAU,EAAEC,UAAU,EAAE0C,OAAO,EACnEG,eAAe,EAAEF,yBAAyB,EAAEC,iBAAiB,CAAC;EAClE,MAAMnC,MAAM,GAAiB,CAACgC,cAAc,EAAEL,KAAK,CAAC;EACpD,IAAItD,IAAI,EAAE;IACR2B,MAAM,CAACO,IAAI,CAAClC,IAAI,CAAC;;EAEnB,IAAI6D,yBAAyB,EAAE;IAC7BlC,MAAM,CAACO,IAAI,CAACjC,sBAAsB,CAAC;;EAErC,IAAI6D,iBAAiB,EAAE;IACrB,MAAMG,eAAe,GAAGlE,OAAO,CAACmE,cAAc,CAC1C,EAAE,EAAE,SAAS,EACbhF,IAAI,CAACiF,iBAAiB,CAACjE,cAAiC,EAAE,SAAS,CAAC,CAAC;IACzEyB,MAAM,CAACO,IAAI,CAAC+B,eAAe,CAAC;IAC5B7C,aAAa,CAACc,IAAI,CAAC+B,eAAe,CAAC;;EAErC,MAAMG,OAAO,GAAGrE,OAAO,CAAC2D,eAAe,CAACM,aAAa,EAAErC,MAAM,EAAE,SAAS,CAAC;EAEzE,MAAMM,QAAQ,GAAGlB,cAAc,GAC3B,CAAC,CAAC,EAAEkC,SAAS,EAAED,QAAQ,EAAElD,QAAQ,CAACgB,WAAW,CAAC,GAC9C,CAAC,CAAC,EAAEhB,QAAQ,CAACgB,WAAW,EAAEmC,SAAS,EAAED,QAAQ,CAAC;EAClD,MAAM7B,GAAG,GACLzB,OAAO,CAAC;IAACiC,MAAM,EAAE;MAAC/B,CAAC,EAAEwE;IAAO,CAAC;IAAErE,OAAO;IAAE6B,KAAK,EAAE;MAACvB,KAAK,EAAE4B;IAAQ;EAAC,CAAC,CAAC;EAEtEb,aAAa,CAACc,IAAI,CAACkC,OAAO,CAAC;EAC3B,KAAK,MAAMzB,CAAC,IAAIvB,aAAa,EAAE;IAC7BrB,OAAO,CAAC6C,6BAA6B,CAACD,CAAC,CAAC;;EAG1C,OAAOxB,GAAG;AACZ"},"metadata":{},"sourceType":"module","externalDependencies":[]}