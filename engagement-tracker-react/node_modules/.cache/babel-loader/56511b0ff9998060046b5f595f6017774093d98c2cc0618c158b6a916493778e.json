{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/ops';\nimport { square } from '../ops/square';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n  constructor(learningRate, rho) {\n    let epsilon = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    super();\n    this.learningRate = learningRate;\n    this.rho = rho;\n    this.epsilon = epsilon;\n    this.accumulatedGrads = [];\n    this.accumulatedUpdates = [];\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: \"\".concat(name, \"/accum_grad\"),\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: \"\".concat(name, \"/accum_var\"),\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n      tidy(() => {\n        const newAccumulatedGrad = add(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));\n        const updates = mul(div(sqrt(add(accumulatedUpdate, this.epsilon)), sqrt(add(accumulatedGrad, this.epsilon))), gradient);\n        const newAccumulatedUpdate = add(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n  dispose() {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedGrads, ...this.accumulatedUpdates];\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedGrads = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedUpdates = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n  }\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n}\n/** @nocollapse */\nAdadeltaOptimizer.className = 'Adadelta'; // Name matters for Python compatibility.\nregisterClass(AdadeltaOptimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","zerosLike","registerClass","Optimizer","AdadeltaOptimizer","constructor","learningRate","rho","epsilon","arguments","length","undefined","accumulatedGrads","accumulatedUpdates","backend","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","concat","variable","gradient","tensor","accumulatedGrad","accumulatedUpdate","newAccumulatedGrad","updates","newAccumulatedUpdate","assign","newValue","incrementIterations","v","getWeights","variables","saveIterations","setWeights","weightValues","extractIterations","variableCount","slice","getConfig","fromConfig","cls","config","className"],"sources":["C:\\Users\\reddy\\Documents\\Projects\\engagement-tracker-react\\node_modules\\@tensorflow\\tfjs-core\\src\\optimizers\\adadelta_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/ops';\nimport {square} from '../ops/square';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'Adadelta';  // Name matters for Python compatibility.\n  private accumulatedGrads: OptimizerVariable[] = [];\n  private accumulatedUpdates: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected rho: number,\n      protected epsilon: number = null) {\n    super();\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accum_grad`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: `${name}/accum_var`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n\n      tidy(() => {\n        const newAccumulatedGrad =\n            add(mul(accumulatedGrad, this.rho),\n                mul(square(gradient), 1 - this.rho));\n\n        const updates =\n            mul(div(sqrt(add(accumulatedUpdate, this.epsilon)),\n                    sqrt(add(accumulatedGrad, this.epsilon))),\n                gradient);\n\n        const newAccumulatedUpdate =\n            add(mul(accumulatedUpdate, this.rho),\n                mul(square(updates), 1 - this.rho));\n\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedGrads, ...this.accumulatedUpdates];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedGrads =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedUpdates =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n}\nregisterClass(AdadeltaOptimizer);\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,IAAI,QAAO,YAAY;AAC/B,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,SAAS,QAAO,mBAAmB;AAC3C,SAAoBC,aAAa,QAA8C,kBAAkB;AAGjG,SAAQC,SAAS,QAA0B,aAAa;AAExD;AACA,OAAM,MAAOC,iBAAkB,SAAQD,SAAS;EAM9CE,YACcC,YAAoB,EAAYC,GAAW,EACrB;IAAA,IAAtBC,OAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;IAClC,KAAK,EAAE;IAFK,KAAAH,YAAY,GAAZA,YAAY;IAAoB,KAAAC,GAAG,GAAHA,GAAG;IACnC,KAAAC,OAAO,GAAPA,OAAO;IALb,KAAAI,gBAAgB,GAAwB,EAAE;IAC1C,KAAAC,kBAAkB,GAAwB,EAAE;IAOlD,IAAIL,OAAO,IAAI,IAAI,EAAE;MACnB,IAAI,CAACA,OAAO,GAAGf,MAAM,CAACqB,OAAO,CAACN,OAAO,EAAE;;EAE3C;EAEAO,cAAcA,CAACC,iBAAiD;IAC9D,MAAMC,aAAa,GAAGC,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAClDA,iBAAiB,CAACI,GAAG,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,GACxCC,MAAM,CAACC,IAAI,CAACR,iBAAiB,CAAC;IAElCC,aAAa,CAACQ,OAAO,CAAC,CAACH,IAAI,EAAEI,CAAC,KAAI;MAChC,MAAMC,KAAK,GAAGlC,MAAM,CAACmC,mBAAmB,CAACN,IAAI,CAAC;MAC9C,MAAMO,SAAS,GAAG,KAAK;MACvB,IAAI,IAAI,CAACjB,gBAAgB,CAACc,CAAC,CAAC,IAAI,IAAI,EAAE;QACpC,IAAI,CAACd,gBAAgB,CAACc,CAAC,CAAC,GAAG;UACzBI,YAAY,KAAAC,MAAA,CAAKT,IAAI,gBAAa;UAClCU,QAAQ,EAAErC,IAAI,CAAC,MAAMM,SAAS,CAAC0B,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAEH,IAAI,IAAI,CAAChB,kBAAkB,CAACa,CAAC,CAAC,IAAI,IAAI,EAAE;QACtC,IAAI,CAACb,kBAAkB,CAACa,CAAC,CAAC,GAAG;UAC3BI,YAAY,KAAAC,MAAA,CAAKT,IAAI,eAAY;UACjCU,QAAQ,EAAErC,IAAI,CAAC,MAAMM,SAAS,CAAC0B,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAGH,MAAMI,QAAQ,GAAGf,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACU,CAAC,CAAC,CAACQ,MAAM,GAC3BlB,iBAAiB,CAACM,IAAI,CAAC;MAC3B,IAAIW,QAAQ,IAAI,IAAI,EAAE;QACpB;;MAGF,MAAME,eAAe,GAAG,IAAI,CAACvB,gBAAgB,CAACc,CAAC,CAAC,CAACM,QAAQ;MACzD,MAAMI,iBAAiB,GAAG,IAAI,CAACvB,kBAAkB,CAACa,CAAC,CAAC,CAACM,QAAQ;MAE7DrC,IAAI,CAAC,MAAK;QACR,MAAM0C,kBAAkB,GACpBzC,GAAG,CAACE,GAAG,CAACqC,eAAe,EAAE,IAAI,CAAC5B,GAAG,CAAC,EAC9BT,GAAG,CAACE,MAAM,CAACiC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC1B,GAAG,CAAC,CAAC;QAE5C,MAAM+B,OAAO,GACTxC,GAAG,CAACD,GAAG,CAACE,IAAI,CAACH,GAAG,CAACwC,iBAAiB,EAAE,IAAI,CAAC5B,OAAO,CAAC,CAAC,EAC1CT,IAAI,CAACH,GAAG,CAACuC,eAAe,EAAE,IAAI,CAAC3B,OAAO,CAAC,CAAC,CAAC,EAC7CyB,QAAQ,CAAC;QAEjB,MAAMM,oBAAoB,GACtB3C,GAAG,CAACE,GAAG,CAACsC,iBAAiB,EAAE,IAAI,CAAC7B,GAAG,CAAC,EAChCT,GAAG,CAACE,MAAM,CAACsC,OAAO,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC/B,GAAG,CAAC,CAAC;QAE3C4B,eAAe,CAACK,MAAM,CAACH,kBAAkB,CAAC;QAC1CD,iBAAiB,CAACI,MAAM,CAACD,oBAAoB,CAAC;QAE9C,MAAME,QAAQ,GAAG7C,GAAG,CAACE,GAAG,CAACwC,OAAO,EAAE,CAAC,IAAI,CAAChC,YAAY,CAAC,EAAEqB,KAAK,CAAC;QAC7DA,KAAK,CAACa,MAAM,CAACC,QAAQ,CAAC;MACxB,CAAC,CAAC;IACJ,CAAC,CAAC;IACF,IAAI,CAACC,mBAAmB,EAAE;EAC5B;EAEAhD,OAAOA,CAAA;IACL,IAAI,IAAI,CAACmB,kBAAkB,IAAI,IAAI,EAAE;MACnCnB,OAAO,CAAC,IAAI,CAACkB,gBAAgB,CAACQ,GAAG,CAACuB,CAAC,IAAIA,CAAC,CAACX,QAAQ,CAAC,CAAC;MACnDtC,OAAO,CAAC,IAAI,CAACmB,kBAAkB,CAACO,GAAG,CAACuB,CAAC,IAAIA,CAAC,CAACX,QAAQ,CAAC,CAAC;;EAEzD;EAEA,MAAMY,UAAUA,CAAA;IACd;IACA,MAAMC,SAAS,GACX,CAAC,GAAG,IAAI,CAACjC,gBAAgB,EAAE,GAAG,IAAI,CAACC,kBAAkB,CAAC;IAC1D,OAAO,CAAC,MAAM,IAAI,CAACiC,cAAc,EAAE,CAAC,CAACf,MAAM,CACvCc,SAAS,CAACzB,GAAG,CAACuB,CAAC,KAAK;MAACrB,IAAI,EAAEqB,CAAC,CAACb,YAAY;MAAEI,MAAM,EAAES,CAAC,CAACX;IAAQ,CAAC,CAAC,CAAC,CAAC;EACvE;EAEA,MAAMe,UAAUA,CAACC,YAA2B;IAC1CA,YAAY,GAAG,MAAM,IAAI,CAACC,iBAAiB,CAACD,YAAY,CAAC;IACzD,MAAME,aAAa,GAAGF,YAAY,CAACtC,MAAM,GAAG,CAAC;IAC7C,MAAMmB,SAAS,GAAG,KAAK;IACvB,IAAI,CAACjB,gBAAgB,GACjBoC,YAAY,CAACG,KAAK,CAAC,CAAC,EAAED,aAAa,CAAC,CAAC9B,GAAG,CAACuB,CAAC,KAAK;MACJb,YAAY,EAAEa,CAAC,CAACrB,IAAI;MACpBU,QAAQ,EAAEW,CAAC,CAACT,MAAM,CAACF,QAAQ,CACvBH,SAAS;KACd,CAAC,CAAC;IAChD,IAAI,CAAChB,kBAAkB,GACnBmC,YAAY,CAACG,KAAK,CAACD,aAAa,EAAEA,aAAa,GAAG,CAAC,CAAC,CAC/C9B,GAAG,CAACuB,CAAC,KAAK;MACJb,YAAY,EAAEa,CAAC,CAACrB,IAAI;MACpBU,QAAQ,EAAEW,CAAC,CAACT,MAAM,CAACF,QAAQ,CAACH,SAAS;KACtC,CAAC,CAAC;EAClB;EAEAuB,SAASA,CAAA;IACP,OAAO;MACL,cAAc,EAAE,IAAI,CAAC9C,YAAY;MACjC,KAAK,EAAE,IAAI,CAACC,GAAG;MACf,SAAS,EAAE,IAAI,CAACC;KACjB;EACH;EAEA;EACA,OAAO6C,UAAUA,CACbC,GAA+B,EAAEC,MAAkB;IACrD,OAAO,IAAID,GAAG,CAACC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,KAAK,CAAC,EAAEA,MAAM,CAAC,SAAS,CAAC,CAAC;EAC1E;;AAnHA;AACOnD,iBAAA,CAAAoD,SAAS,GAAG,UAAU,CAAC,CAAE;AAoHlCtD,aAAa,CAACE,iBAAiB,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}