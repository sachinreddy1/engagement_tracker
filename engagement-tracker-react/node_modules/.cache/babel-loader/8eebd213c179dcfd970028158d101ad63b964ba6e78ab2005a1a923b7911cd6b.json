{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  constructor(learningRate) {\n    let decay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n    let momentum = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.0;\n    let epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    let centered = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : false;\n    super();\n    this.learningRate = learningRate;\n    this.decay = decay;\n    this.momentum = momentum;\n    this.epsilon = epsilon;\n    this.accumulatedMeanSquares = [];\n    this.accumulatedMoments = [];\n    this.accumulatedMeanGrads = [];\n    this.centered = centered;\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(\"learningRate for RMSPropOptimizer must be defined.\");\n    }\n  }\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: \"\".concat(name, \"/rms\"),\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: \"\".concat(name, \"/momentum\"),\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: \"\".concat(name, \"/mg\"),\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n          const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n  dispose() {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedMoments = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    if (this.centered) {\n      this.accumulatedMeanGrads = weightValues.slice(variableCount * 2, variableCount * 3).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    }\n  }\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n  /** @nocollapse */\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n  }\n}\n/** @nocollapse */\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\nregisterClass(RMSPropOptimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","sub","zerosLike","registerClass","Optimizer","RMSPropOptimizer","constructor","learningRate","decay","arguments","length","undefined","momentum","epsilon","centered","accumulatedMeanSquares","accumulatedMoments","accumulatedMeanGrads","backend","Error","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","originalName","concat","variable","gradient","tensor","accumulatedMeanSquare","newAccumulatedMeanSquare","accumulatedMeanGrad","newAccumulatedMeanGrad","gradContribution","newAccumulatedMoments","assign","newValue","incrementIterations","v","getWeights","variables","push","saveIterations","setWeights","weightValues","extractIterations","variableCount","slice","getConfig","fromConfig","cls","config","className"],"sources":["C:\\Users\\reddy\\Documents\\Projects\\Engagement Tracker\\engagement-tracker-react\\node_modules\\@tensorflow\\tfjs-core\\src\\optimizers\\rmsprop_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'RMSProp';  // Note: Name matters for Python compatibility.\n  private centered: boolean;\n\n  private accumulatedMeanSquares: OptimizerVariable[] = [];\n  private accumulatedMoments: OptimizerVariable[] = [];\n  private accumulatedMeanGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected decay = 0.9,\n      protected momentum = 0.0, protected epsilon: number = null,\n      centered = false) {\n    super();\n\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare =\n            add(mul(accumulatedMeanSquare, this.decay),\n                mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad =\n              add(mul(accumulatedMeanGrad, this.decay),\n                  mul(gradient, 1 - this.decay));\n\n          const gradContribution =\n              div(mul(gradient, this.learningRate),\n                  sqrt(\n                      sub(newAccumulatedMeanSquare,\n                          add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum), gradContribution);\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare =\n              add(mul(accumulatedMeanSquare, this.decay),\n                  mul(square(gradient), 1 - this.decay));\n\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum),\n                  div(mul(gradient, this.learningRate),\n                      sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount =\n        this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedMoments =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n    if (this.centered) {\n      this.accumulatedMeanGrads =\n          weightValues.slice(variableCount * 2, variableCount * 3)\n              .map(v => ({\n                     originalName: v.name,\n                     variable: v.tensor.variable(trainable)\n                   }));\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['decay'], config['momentum'],\n        config['epsilon'], config['centered']);\n  }\n}\nregisterClass(RMSPropOptimizer);\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,IAAI,QAAO,aAAa;AAChC,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,SAAS,QAAO,mBAAmB;AAC3C,SAAoBC,aAAa,QAA8C,kBAAkB;AAGjG,SAAQC,SAAS,QAA0B,aAAa;AAExD;AACA,OAAM,MAAOC,gBAAiB,SAAQD,SAAS;EAS7CE,YACcC,YAAoB,EAEd;IAAA,IAF0BC,KAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAQ,GAAG;IAAA,IAC3CG,QAAA,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAW,GAAG;IAAA,IAAYI,OAAA,GAAAJ,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;IAAA,IAC1DK,QAAQ,GAAAL,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;IAClB,KAAK,EAAE;IAHK,KAAAF,YAAY,GAAZA,YAAY;IAAoB,KAAAC,KAAK,GAALA,KAAK;IACrC,KAAAI,QAAQ,GAARA,QAAQ;IAAkB,KAAAC,OAAO,GAAPA,OAAO;IANvC,KAAAE,sBAAsB,GAAwB,EAAE;IAChD,KAAAC,kBAAkB,GAAwB,EAAE;IAC5C,KAAAC,oBAAoB,GAAwB,EAAE;IAQpD,IAAI,CAACH,QAAQ,GAAGA,QAAQ;IAExB,IAAID,OAAO,IAAI,IAAI,EAAE;MACnB,IAAI,CAACA,OAAO,GAAGpB,MAAM,CAACyB,OAAO,CAACL,OAAO,EAAE;;IAEzC,IAAIN,YAAY,IAAI,IAAI,EAAE;MACxB,MAAM,IAAIY,KAAK,qDAAqD,CAAC;;EAEzE;EAEAC,cAAcA,CAACC,iBAA+C;IAC5D,MAAMC,aAAa,GAAGC,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAClDA,iBAAiB,CAACI,GAAG,CAACC,IAAI,IAAIA,IAAI,CAACC,IAAI,CAAC,GACxCC,MAAM,CAACC,IAAI,CAACR,iBAAiB,CAAC;IAElCC,aAAa,CAACQ,OAAO,CAAC,CAACH,IAAI,EAAEI,CAAC,KAAI;MAChC,MAAMC,KAAK,GAAGvC,MAAM,CAACwC,mBAAmB,CAACN,IAAI,CAAC;MAC9C,MAAMO,SAAS,GAAG,KAAK;MACvB,IAAI,IAAI,CAACnB,sBAAsB,CAACgB,CAAC,CAAC,IAAI,IAAI,EAAE;QAC1C,IAAI,CAAChB,sBAAsB,CAACgB,CAAC,CAAC,GAAG;UAC/BI,YAAY,KAAAC,MAAA,CAAKT,IAAI,SAAM;UAC3BU,QAAQ,EAAE1C,IAAI,CAAC,MAAMO,SAAS,CAAC8B,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAEH,IAAI,IAAI,CAAClB,kBAAkB,CAACe,CAAC,CAAC,IAAI,IAAI,EAAE;QACtC,IAAI,CAACf,kBAAkB,CAACe,CAAC,CAAC,GAAG;UAC3BI,YAAY,KAAAC,MAAA,CAAKT,IAAI,cAAW;UAChCU,QAAQ,EAAE1C,IAAI,CAAC,MAAMO,SAAS,CAAC8B,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAEH,IAAI,IAAI,CAACjB,oBAAoB,CAACc,CAAC,CAAC,IAAI,IAAI,IAAI,IAAI,CAACjB,QAAQ,EAAE;QACzD,IAAI,CAACG,oBAAoB,CAACc,CAAC,CAAC,GAAG;UAC7BI,YAAY,KAAAC,MAAA,CAAKT,IAAI,QAAK;UAC1BU,QAAQ,EAAE1C,IAAI,CAAC,MAAMO,SAAS,CAAC8B,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;SAC1D;;MAGH,MAAMI,QAAQ,GAAGf,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACU,CAAC,CAAC,CAACQ,MAAM,GAC3BlB,iBAAiB,CAACM,IAAI,CAAC;MAC3B,IAAIW,QAAQ,IAAI,IAAI,EAAE;QACpB;;MAGF,MAAME,qBAAqB,GAAG,IAAI,CAACzB,sBAAsB,CAACgB,CAAC,CAAC,CAACM,QAAQ;MACrE,MAAMrB,kBAAkB,GAAG,IAAI,CAACA,kBAAkB,CAACe,CAAC,CAAC,CAACM,QAAQ;MAC9D1C,IAAI,CAAC,MAAK;QACR,MAAM8C,wBAAwB,GAC1B7C,GAAG,CAACE,GAAG,CAAC0C,qBAAqB,EAAE,IAAI,CAAChC,KAAK,CAAC,EACtCV,GAAG,CAACE,MAAM,CAACsC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC9B,KAAK,CAAC,CAAC;QAE9C,IAAI,IAAI,CAACM,QAAQ,EAAE;UACjB,MAAM4B,mBAAmB,GAAG,IAAI,CAACzB,oBAAoB,CAACc,CAAC,CAAC,CAACM,QAAQ;UACjE;UACA,MAAMM,sBAAsB,GACxB/C,GAAG,CAACE,GAAG,CAAC4C,mBAAmB,EAAE,IAAI,CAAClC,KAAK,CAAC,EACpCV,GAAG,CAACwC,QAAQ,EAAE,CAAC,GAAG,IAAI,CAAC9B,KAAK,CAAC,CAAC;UAEtC,MAAMoC,gBAAgB,GAClB/C,GAAG,CAACC,GAAG,CAACwC,QAAQ,EAAE,IAAI,CAAC/B,YAAY,CAAC,EAChCR,IAAI,CACAE,GAAG,CAACwC,wBAAwB,EACxB7C,GAAG,CAACI,MAAM,CAAC2C,sBAAsB,CAAC,EAAE,IAAI,CAAC9B,OAAO,CAAC,CAAC,CAAC,CAAC;UACpE,MAAMgC,qBAAqB,GACvBjD,GAAG,CAACE,GAAG,CAACkB,kBAAkB,EAAE,IAAI,CAACJ,QAAQ,CAAC,EAAEgC,gBAAgB,CAAC;UAEjEJ,qBAAqB,CAACM,MAAM,CAACL,wBAAwB,CAAC;UACtDC,mBAAmB,CAACI,MAAM,CAACH,sBAAsB,CAAC;UAClD3B,kBAAkB,CAAC8B,MAAM,CAACD,qBAAqB,CAAC;UAEhD,MAAME,QAAQ,GAAG9C,GAAG,CAAC+B,KAAK,EAAEa,qBAAqB,CAAC;UAClDb,KAAK,CAACc,MAAM,CAACC,QAAQ,CAAC;SACvB,MAAM;UACL;UACA,MAAMN,wBAAwB,GAC1B7C,GAAG,CAACE,GAAG,CAAC0C,qBAAqB,EAAE,IAAI,CAAChC,KAAK,CAAC,EACtCV,GAAG,CAACE,MAAM,CAACsC,QAAQ,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC9B,KAAK,CAAC,CAAC;UAE9C,MAAMqC,qBAAqB,GACvBjD,GAAG,CAACE,GAAG,CAACkB,kBAAkB,EAAE,IAAI,CAACJ,QAAQ,CAAC,EACtCf,GAAG,CAACC,GAAG,CAACwC,QAAQ,EAAE,IAAI,CAAC/B,YAAY,CAAC,EAChCR,IAAI,CAACH,GAAG,CAAC6C,wBAAwB,EAAE,IAAI,CAAC5B,OAAO,CAAC,CAAC,CAAC,CAAC;UAE/D2B,qBAAqB,CAACM,MAAM,CAACL,wBAAwB,CAAC;UACtDzB,kBAAkB,CAAC8B,MAAM,CAACD,qBAAqB,CAAC;UAEhD,MAAME,QAAQ,GAAG9C,GAAG,CAAC+B,KAAK,EAAEa,qBAAqB,CAAC;UAClDb,KAAK,CAACc,MAAM,CAACC,QAAQ,CAAC;;MAE1B,CAAC,CAAC;IACJ,CAAC,CAAC;IACF,IAAI,CAACC,mBAAmB,EAAE;EAC5B;EAEAtD,OAAOA,CAAA;IACL,IAAI,IAAI,CAACqB,sBAAsB,IAAI,IAAI,EAAE;MACvCrB,OAAO,CAAC,IAAI,CAACqB,sBAAsB,CAACU,GAAG,CAACwB,CAAC,IAAIA,CAAC,CAACZ,QAAQ,CAAC,CAAC;;IAE3D,IAAI,IAAI,CAACpB,oBAAoB,IAAI,IAAI,IAAI,IAAI,CAACH,QAAQ,EAAE;MACtDpB,OAAO,CAAC,IAAI,CAACuB,oBAAoB,CAACQ,GAAG,CAACwB,CAAC,IAAIA,CAAC,CAACZ,QAAQ,CAAC,CAAC;;IAEzD,IAAI,IAAI,CAACrB,kBAAkB,IAAI,IAAI,EAAE;MACnCtB,OAAO,CAAC,IAAI,CAACsB,kBAAkB,CAACS,GAAG,CAACwB,CAAC,IAAIA,CAAC,CAACZ,QAAQ,CAAC,CAAC;;EAEzD;EAEA,MAAMa,UAAUA,CAAA;IACd;IACA,MAAMC,SAAS,GACX,CAAC,GAAG,IAAI,CAACpC,sBAAsB,EAAE,GAAG,IAAI,CAACC,kBAAkB,CAAC;IAChE,IAAI,IAAI,CAACF,QAAQ,EAAE;MACjBqC,SAAS,CAACC,IAAI,CAAC,GAAG,IAAI,CAACnC,oBAAoB,CAAC;;IAE9C,OAAO,CAAC,MAAM,IAAI,CAACoC,cAAc,EAAE,CAAC,CAACjB,MAAM,CACvCe,SAAS,CAAC1B,GAAG,CAACwB,CAAC,KAAK;MAACtB,IAAI,EAAEsB,CAAC,CAACd,YAAY;MAAEI,MAAM,EAAEU,CAAC,CAACZ;IAAQ,CAAC,CAAC,CAAC,CAAC;EACvE;EAEA,MAAMiB,UAAUA,CAACC,YAA2B;IAC1CA,YAAY,GAAG,MAAM,IAAI,CAACC,iBAAiB,CAACD,YAAY,CAAC;IACzD,MAAME,aAAa,GACf,IAAI,CAAC3C,QAAQ,GAAGyC,YAAY,CAAC7C,MAAM,GAAG,CAAC,GAAG6C,YAAY,CAAC7C,MAAM,GAAG,CAAC;IACrE,MAAMwB,SAAS,GAAG,KAAK;IACvB,IAAI,CAACnB,sBAAsB,GACvBwC,YAAY,CAACG,KAAK,CAAC,CAAC,EAAED,aAAa,CAAC,CAAChC,GAAG,CAACwB,CAAC,KAAK;MACJd,YAAY,EAAEc,CAAC,CAACtB,IAAI;MACpBU,QAAQ,EAAEY,CAAC,CAACV,MAAM,CAACF,QAAQ,CACvBH,SAAS;KACd,CAAC,CAAC;IAChD,IAAI,CAAClB,kBAAkB,GACnBuC,YAAY,CAACG,KAAK,CAACD,aAAa,EAAEA,aAAa,GAAG,CAAC,CAAC,CAC/ChC,GAAG,CAACwB,CAAC,KAAK;MACJd,YAAY,EAAEc,CAAC,CAACtB,IAAI;MACpBU,QAAQ,EAAEY,CAAC,CAACV,MAAM,CAACF,QAAQ,CAACH,SAAS;KACtC,CAAC,CAAC;IAChB,IAAI,IAAI,CAACpB,QAAQ,EAAE;MACjB,IAAI,CAACG,oBAAoB,GACrBsC,YAAY,CAACG,KAAK,CAACD,aAAa,GAAG,CAAC,EAAEA,aAAa,GAAG,CAAC,CAAC,CACnDhC,GAAG,CAACwB,CAAC,KAAK;QACJd,YAAY,EAAEc,CAAC,CAACtB,IAAI;QACpBU,QAAQ,EAAEY,CAAC,CAACV,MAAM,CAACF,QAAQ,CAACH,SAAS;OACtC,CAAC,CAAC;;EAEpB;EAEAyB,SAASA,CAAA;IACP,OAAO;MACL,cAAc,EAAE,IAAI,CAACpD,YAAY;MACjC,OAAO,EAAE,IAAI,CAACC,KAAK;MACnB,UAAU,EAAE,IAAI,CAACI,QAAQ;MACzB,SAAS,EAAE,IAAI,CAACC,OAAO;MACvB,UAAU,EAAE,IAAI,CAACC;KAClB;EACH;EAEA;EACA,OAAO8C,UAAUA,CACbC,GAA+B,EAAEC,MAAkB;IACrD,OAAO,IAAID,GAAG,CACVC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,EAAEA,MAAM,CAAC,UAAU,CAAC,EAC3DA,MAAM,CAAC,SAAS,CAAC,EAAEA,MAAM,CAAC,UAAU,CAAC,CAAC;EAC5C;;AA9KA;AACOzD,gBAAA,CAAA0D,SAAS,GAAG,SAAS,CAAC,CAAE;AA+KjC5D,aAAa,CAACE,gBAAgB,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}